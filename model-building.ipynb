{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c03c6f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:03:05.665472Z",
     "iopub.status.busy": "2022-11-29T12:03:05.664819Z",
     "iopub.status.idle": "2022-11-29T12:03:12.510469Z",
     "shell.execute_reply": "2022-11-29T12:03:12.509122Z"
    },
    "papermill": {
     "duration": 6.855027,
     "end_time": "2022-11-29T12:03:12.513578",
     "exception": false,
     "start_time": "2022-11-29T12:03:05.658551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import tensorflow as tf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f60500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:03:12.522225Z",
     "iopub.status.busy": "2022-11-29T12:03:12.521210Z",
     "iopub.status.idle": "2022-11-29T12:03:16.992772Z",
     "shell.execute_reply": "2022-11-29T12:03:16.991209Z"
    },
    "papermill": {
     "duration": 4.479059,
     "end_time": "2022-11-29T12:03:16.995895",
     "exception": false,
     "start_time": "2022-11-29T12:03:12.516836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 12:03:16.940737: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "custom_objects={'TFDistilBertModel': transformers.TFDistilBertModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b383ebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:03:17.007689Z",
     "iopub.status.busy": "2022-11-29T12:03:17.007205Z",
     "iopub.status.idle": "2022-11-29T12:03:51.583876Z",
     "shell.execute_reply": "2022-11-29T12:03:51.582927Z"
    },
    "papermill": {
     "duration": 34.586018,
     "end_time": "2022-11-29T12:03:51.586635",
     "exception": false,
     "start_time": "2022-11-29T12:03:17.000617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 12:03:17.081686: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "cohesion_model = tf.keras.models.load_model('../input/cohesion/cohesion.h5', custom_objects)\n",
    "syntax_model = tf.keras.models.load_model('../input/syntax/syntax.h5', custom_objects)\n",
    "vocabulary_model = tf.keras.models.load_model('../input/vocabulary/vocabulary.h5', custom_objects)\n",
    "phraseology_model = tf.keras.models.load_model('../input/phraseology/phraseology.h5', custom_objects)\n",
    "grammar_model = tf.keras.models.load_model('../input/grammar/grammar.h5', custom_objects)\n",
    "conventions_model = tf.keras.models.load_model('../input/conventions/conventions.h5', custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910b75d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:03:51.595444Z",
     "iopub.status.busy": "2022-11-29T12:03:51.594156Z",
     "iopub.status.idle": "2022-11-29T12:03:51.603826Z",
     "shell.execute_reply": "2022-11-29T12:03:51.602578Z"
    },
    "papermill": {
     "duration": 0.017717,
     "end_time": "2022-11-29T12:03:51.607497",
     "exception": false,
     "start_time": "2022-11-29T12:03:51.589780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for layer in cohesion_model.layers:\n",
    "    layer._name = layer._name + str(\"_cohesion\")\n",
    "for layer in syntax_model.layers:\n",
    "    layer._name = layer._name + str(\"_syntax\")\n",
    "for layer in vocabulary_model.layers:\n",
    "    layer._name = layer._name + str(\"_vocabulary\")\n",
    "for layer in phraseology_model.layers:\n",
    "    layer._name = layer._name + str(\"_phraseology\")\n",
    "for layer in grammar_model.layers:\n",
    "    layer._name = layer._name + str(\"_grammar\")\n",
    "for layer in conventions_model.layers:\n",
    "    layer._name = layer._name + str(\"_conventions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d60647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:03:51.615929Z",
     "iopub.status.busy": "2022-11-29T12:03:51.615522Z",
     "iopub.status.idle": "2022-11-29T12:03:56.854573Z",
     "shell.execute_reply": "2022-11-29T12:03:56.853322Z"
    },
    "papermill": {
     "duration": 5.246237,
     "end_time": "2022-11-29T12:03:56.857436",
     "exception": false,
     "start_time": "2022-11-29T12:03:51.611199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at ../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "input_ids_layer = tf.keras.Input(shape=512, dtype=tf.int32)\n",
    "attention_mask_layer = tf.keras.Input(shape=512, dtype=tf.int32)\n",
    "base_model = transformers.TFDistilBertModel.from_pretrained('../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased')\n",
    "base_model_output = base_model(input_ids=input_ids_layer, attention_mask = attention_mask_layer)\n",
    "\n",
    "cohesion_pooling = cohesion_model.layers[3]\n",
    "cohesion_pooling_output = cohesion_pooling(base_model_output.last_hidden_state)\n",
    "cohesion_output = cohesion_model.layers[4]\n",
    "cohesion_output_output = cohesion_output(cohesion_pooling_output)\n",
    "\n",
    "syntax_pooling = syntax_model.layers[3]\n",
    "syntax_pooling_output = syntax_pooling(base_model_output.last_hidden_state)\n",
    "syntax_output = syntax_model.layers[4]\n",
    "syntax_output_output = syntax_output(syntax_pooling_output)\n",
    "\n",
    "vocabulary_pooling = vocabulary_model.layers[3]\n",
    "vocabulary_pooling_output = vocabulary_pooling(base_model_output.last_hidden_state)\n",
    "vocabulary_output = vocabulary_model.layers[4]\n",
    "vocabulary_output_output = vocabulary_output(vocabulary_pooling_output)\n",
    "\n",
    "phraseology_pooling = phraseology_model.layers[3]\n",
    "phraseology_pooling_output = phraseology_pooling(base_model_output.last_hidden_state)\n",
    "phraseology_output = phraseology_model.layers[4]\n",
    "phraseology_output_output = phraseology_output(phraseology_pooling_output)\n",
    "\n",
    "grammar_pooling = grammar_model.layers[3]\n",
    "grammar_pooling_output = grammar_pooling(base_model_output.last_hidden_state)\n",
    "grammar_output = grammar_model.layers[4]\n",
    "grammar_output_output = grammar_output(grammar_pooling_output)\n",
    "\n",
    "conventions_pooling = conventions_model.layers[3]\n",
    "conventions_pooling_output = conventions_pooling(base_model_output.last_hidden_state)\n",
    "conventions_output = conventions_model.layers[4]\n",
    "conventions_output_output = conventions_output(conventions_pooling_output)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_ids_layer, attention_mask_layer],\n",
    "    outputs=[cohesion_output_output, syntax_output_output, vocabulary_output_output,\n",
    "            phraseology_output_output, grammar_output_output, conventions_output_output],)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003), \n",
    "    loss='mse',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0efa8636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:03:56.866743Z",
     "iopub.status.busy": "2022-11-29T12:03:56.866344Z",
     "iopub.status.idle": "2022-11-29T12:03:56.880912Z",
     "shell.execute_reply": "2022-11-29T12:03:56.879674Z"
    },
    "papermill": {
     "duration": 0.023192,
     "end_time": "2022-11-29T12:03:56.884215",
     "exception": false,
     "start_time": "2022-11-29T12:03:56.861023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_6 (TFDisti TFBaseModelOutput(la 66362880    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_cohesi (None, 768)          0           tf_distil_bert_model_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_syntax (None, 768)          0           tf_distil_bert_model_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_vocabu (None, 768)          0           tf_distil_bert_model_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_phrase (None, 768)          0           tf_distil_bert_model_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_gramma (None, 768)          0           tf_distil_bert_model_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_conven (None, 768)          0           tf_distil_bert_model_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_cohesion (Dense)          (None, 1)            769         global_average_pooling1d_cohesion\n",
      "__________________________________________________________________________________________________\n",
      "dense_syntax (Dense)            (None, 1)            769         global_average_pooling1d_syntax[1\n",
      "__________________________________________________________________________________________________\n",
      "dense_vocabulary (Dense)        (None, 1)            769         global_average_pooling1d_vocabula\n",
      "__________________________________________________________________________________________________\n",
      "dense_phraseology (Dense)       (None, 1)            769         global_average_pooling1d_phraseol\n",
      "__________________________________________________________________________________________________\n",
      "dense_grammar (Dense)           (None, 1)            769         global_average_pooling1d_grammar[\n",
      "__________________________________________________________________________________________________\n",
      "dense_conventions (Dense)       (None, 1)            769         global_average_pooling1d_conventi\n",
      "==================================================================================================\n",
      "Total params: 66,367,494\n",
      "Trainable params: 66,367,494\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57309a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:03:56.893902Z",
     "iopub.status.busy": "2022-11-29T12:03:56.893494Z",
     "iopub.status.idle": "2022-11-29T12:03:57.547369Z",
     "shell.execute_reply": "2022-11-29T12:03:57.546051Z"
    },
    "papermill": {
     "duration": 0.661451,
     "end_time": "2022-11-29T12:03:57.550215",
     "exception": false,
     "start_time": "2022-11-29T12:03:56.888764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer_config.json',\n",
       " './special_tokens_map.json',\n",
       " './vocab.txt',\n",
       " './added_tokens.json',\n",
       " './tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('model.h5')\n",
    "config = transformers.DistilBertConfig.from_pretrained('../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased')\n",
    "config.save_pretrained('./')\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained('../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased')\n",
    "tokenizer.save_pretrained('./')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 65.372756,
   "end_time": "2022-11-29T12:04:01.050524",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-29T12:02:55.677768",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
